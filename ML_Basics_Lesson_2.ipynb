{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\">\n",
    "    <ul class=\"toc\"><li><a href=\"#Machine-Learning-Fundamentals\">I. Machine Learning Fundamentals<a class=\"anchor-link\" href=\"#Machine-Learning-Fundamentals\">¶</a></a></li><li><a href=\"#Table-of-Contents\">II. Table of Contents<a class=\"anchor-link\" href=\"#Table-of-Contents\">¶</a></a></li><ul class=\"toc\"><li><a href=\"#What-is-Machine-Learning\">I. What is Machine Learning<a class=\"anchor-link\" href=\"#What-is-Machine-Learning\">¶</a></a></li><li><a href=\"#What-types-of-Machine-Learning-are-there\">II. What types of Machine Learning are there<a class=\"anchor-link\" href=\"#What-types-of-Machine-Learning-are-there\">¶</a></a></li><ul class=\"toc\"><li><a href=\"#Supervised-Learning\">I. Supervised Learning<a class=\"anchor-link\" href=\"#Supervised-Learning\">¶</a></a></li><ul class=\"toc\"><li><a href=\"#Classification\">I. Classification<a class=\"anchor-link\" href=\"#Classification\">¶</a></a></li><li><a href=\"#Regression\">II. Regression<a class=\"anchor-link\" href=\"#Regression\">¶</a></a></li></ul><li><a href=\"#Unsupervised-Learning\">II. Unsupervised Learning<a class=\"anchor-link\" href=\"#Unsupervised-Learning\">¶</a></a></li><ul class=\"toc\"><li><a href=\"#Clustering\">I. Clustering<a class=\"anchor-link\" href=\"#Clustering\">¶</a></a></li><li><a href=\"#Anomaly-detection\">II. Anomaly detection<a class=\"anchor-link\" href=\"#Anomaly-detection\">¶</a></a></li></ul><li><a href=\"#Reinforcement-Learning\">III. Reinforcement Learning<a class=\"anchor-link\" href=\"#Reinforcement-Learning\">¶</a></a></li></ul><li><a href=\"#The-steps-to-building-a-Model\">III. The steps to building a Model<a class=\"anchor-link\" href=\"#The-steps-to-building-a-Model\">¶</a></a></li><ul class=\"toc\"><li><a href=\"#Get-Data\">I. Get Data<a class=\"anchor-link\" href=\"#Get-Data\">¶</a></a></li><li><a href=\"#Clean%2C-Prepare-%26-Manipulate-Data\">II. Clean, Prepare &amp; Manipulate Data<a class=\"anchor-link\" href=\"#Clean,-Prepare-&amp;-Manipulate-Data\">¶</a></a></li><ul class=\"toc\"><li><a href=\"#Missing-Data\">I. Missing Data<a class=\"anchor-link\" href=\"#Missing-Data\">¶</a></a></li><ul class=\"toc\"><li><a href=\"#Example-of-Marking-Missing-Data\">I. Example of Marking Missing Data<a class=\"anchor-link\" href=\"#Example-of-Marking-Missing-Data\">¶</a></a></li></ul><li><a href=\"#Data-Normalization\">II. Data Normalization<a class=\"anchor-link\" href=\"#Data-Normalization\">¶</a></a></li></ul><li><a href=\"#Data-Categorization\">III. Data Categorization<a class=\"anchor-link\" href=\"#Data-Categorization\">¶</a></a></li><li><a href=\"#Data-Balancing\">IV. Data Balancing<a class=\"anchor-link\" href=\"#Data-Balancing\">¶</a></a></li><li><a href=\"#Dimensionality-reduction\">V. Dimensionality reduction<a class=\"anchor-link\" href=\"#Dimensionality-reduction\">¶</a></a></li><li><a href=\"#Feautre-extraction\">VI. Feautre extraction<a class=\"anchor-link\" href=\"#Feautre-extraction\">¶</a></a></li><li><a href=\"#Train-Model\">VII. Train Model<a class=\"anchor-link\" href=\"#Train-Model\">¶</a></a></li><li><a href=\"#Test-Data\">VIII. Test Data<a class=\"anchor-link\" href=\"#Test-Data\">¶</a></a></li><li><a href=\"#Improve\">IX. Improve<a class=\"anchor-link\" href=\"#Improve\">¶</a></a></li></ul><li><a href=\"#Common-Pitfalls\">IV. Common Pitfalls<a class=\"anchor-link\" href=\"#Common-Pitfalls\">¶</a></a></li><ul class=\"toc\"><li><a href=\"#Overfitting\">I. Overfitting<a class=\"anchor-link\" href=\"#Overfitting\">¶</a></a></li></ul></ul></ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Machine Learning\n",
    "Machine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it to learn for themselves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What types of Machine Learning are there\n",
    "There are many types of machine learning, the types differ based on the required solution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "#### Classification\n",
    "Machine learning is a field of study and is concerned with algorithms that learn from examples.\n",
    "Classification is a task that requires the use of machine learning algorithms that learn how to assign a class label to examples from the problem domain. An easy to understand example is classifying emails as “spam” or “not spam.”\n",
    "<p align=center>\n",
    "    <img src=\"./Assets/classificationEx.png\" width=400>\n",
    "</p>\n",
    "\n",
    "#### Regression\n",
    "Regression analysis consists of a set of machine learning methods that allow us to predict a continuous outcome variable (y) based on the value of one or multiple predictor variables (x).\n",
    "Fundamentally, classification is about predicting a label and regression is about predicting a quantity. \n",
    "<p align=center>\n",
    "    <img src=\"./Assets/regressionEx.png\" width=400>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning \n",
    "#### Clustering\n",
    "\n",
    "Clustering is a Machine Learning technique that involves the grouping of data points. Given a set of data points, we can use a clustering algorithm to classify each data point into a specific group. In theory, data points that are in the same group should have similar properties and/or features, while data points in different groups should have highly dissimilar properties and/or features. Clustering is a method of unsupervised learning and is a common technique for statistical data analysis used in many fields.\n",
    "<p align=center>\n",
    "    <img src=\"./Assets/clusteringEx.png\" width=400>\n",
    "</p>\n",
    "\n",
    "#### Anomaly detection\n",
    "Anomaly detection is any process that finds the outliers of a dataset; those items that don’t belong. These anomalies might point to unusual network traffic, uncover a sensor on the fritz, or simply identify data for cleaning, before analysis.\n",
    "<p align=center>\n",
    "    <img src=\"./Assets/anomalyEx.jpeg\" width=400>\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning\n",
    "Reinforcement learning is an area of Machine Learning. It is about taking suitable action to maximize reward in a particular situation. It is employed by various software and machines to find the best possible behavior or path it should take in a specific situation.\n",
    "<p align=center>\n",
    "    <img src=\"./Assets/reinforcementEx.png\" width=400>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=center>\n",
    "    <img src=\"./Assets/scikit-cheatsheet.png\" width=600>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The steps to building a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=center>\n",
    "    <img src=\"./Assets/ML_Steps_Overview.png\" width=400>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean, Prepare & Manipulate Data\n",
    "#### Split Test & Train\n",
    "The simplest way to split the modelling dataset into training and testing sets is to assign 2/3 data points to the former and the remaining one-third to the latter. Therefore, we train the model using the training set and then apply the model to the test set. In this way, we can evaluate the performance of our model.\n",
    "#### Missing Data\n",
    "Sometimes our data has missing values and we need to preform an <b>imputation of missing data </b>.\n",
    "\n",
    "Some common ways are:\n",
    "- Mean (Use this when we want the average)\n",
    "- Median (Use this when we want a median maybe like salary when there are lots of outliers)\n",
    "- Most Probable (Use this when we want most probable maybe reccomendation system)\n",
    "- Nan (Will just be acknoledged by the model as not a number)\n",
    "\n",
    "\n",
    "\n",
    "<p align=center>\n",
    "    <img src=\"./Assets/imputationEx.png\" width=400>\n",
    "</p>\n",
    "A common mistake is just to put zero which is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example of Marking Missing Data\n",
    "https://scikit-learn.org/stable/modules/impute.html#impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.         2.        ]\n",
      " [6.         3.66666667]\n",
      " [7.         6.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit([[1, 2], [np.nan, 3], [7, 6]])\n",
    "X = [[np.nan, 2], [6, np.nan], [7, 6]]\n",
    "print(imp.transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Normalization\n",
    "The purpose of normalization is to transform data in a way that they are either dimensionless and/or have similar distributions. This process of normalization is known by other names such as standardization, feature scaling etc. Normalization is an essential step in data pre-processing in any machine learning application and model fitting.\n",
    "\n",
    "Here are 2 popular techniques:\n",
    "\n",
    "- Mean normalization: This method uses the mean of the observations in the transformation process:\n",
    "\n",
    "<p align=center>\n",
    "    <img src=\"https://miro.medium.com/max/295/1*YowZdhWKSYsjGr81zkz_Kw.png\" width=200>\n",
    "</p>\n",
    "\n",
    "\n",
    "- Z-score normalization: Also known as standardization, this technic uses Z-score or “standard score”. It is widely used in machine learning algorithms such as SVM and logistic regression:\n",
    "\n",
    "<p align=center>\n",
    "    <img src=\"https://miro.medium.com/max/136/1*uANJsFWEqyQ0Gh8_Rs1-KA.png\" width=200>\n",
    "</p>\n",
    "\n",
    "Here, z is the standard score, µ is the population mean and ϭ is the population standard deviation.\n",
    "\n",
    "- L1,L2 Standardization\n",
    "\n",
    "L1 is √x1^1 + √x2^1 .... + √xn^1 = just the sum\n",
    "\n",
    "L2 is  √x1^2 + √x2^2 .... + √xn^2 (we usually use this)\n",
    "\n",
    "L∞ is the maximun of them\n",
    "```python\n",
    ">>> X = [[ 1., -1.,  2.],\n",
    "...      [ 2.,  0.,  0.],\n",
    "...      [ 0.,  1., -1.]]\n",
    ">>> X_normalized = preprocessing.normalize(X, norm='l2')\n",
    "```\n",
    "\n",
    "- Unit Length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection\n",
    "Feature selection is the process of reducing the number of input variables when developing a predictive model. It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Categorization\n",
    "Data categorization is a way of puting data in groups with labels so the model can associate the data and its group.\n",
    "\n",
    "- <b> One Hot Encoding <b/>\n",
    "\n",
    "A one hot encoding is a representation of categorical variables as binary vectors.\n",
    "This first requires that the categorical values be mapped to integer values.\n",
    "Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1.\n",
    "\n",
    "Example:\n",
    "\n",
    "Assume we have a sequence of labels with the values ‘red’ and ‘green’.\n",
    "\n",
    "We can assign ‘red’ an integer value of 0 and ‘green’ the integer value of 1. As long as we always assign these numbers to these labels, this is called an integer encoding. Consistency is important so that we can invert the encoding later and get labels back from integer values, such as in the case of making a prediction.\n",
    "\n",
    "Next, we can create a binary vector to represent each integer value. The vector will have a length of 2 for the 2 possible integer values.\n",
    "\n",
    "The ‘red’ label encoded as a 0 will be represented with a binary vector `[1, 0]` where the zeroth index is marked with a value of 1. In turn, the ‘green’ label encoded as a 1 will be represented with a binary vector `[0, 1]` where the first index is marked with a value of 1.\n",
    "\n",
    "If we had the sequence:\n",
    "\n",
    "`'red', 'red', 'green'`\n",
    "\n",
    "We could represent it with the integer encoding:\n",
    "\n",
    "`0, 0, 1`\n",
    "\n",
    "And the one hot encoding of:\n",
    "\n",
    "`[1, 0]`\n",
    "`[1, 0]`\n",
    "`[0, 1]`\n",
    "\n",
    "\n",
    "<p align=center>\n",
    "    <img src=\"./Assets/dataCatEx.png\" width=400>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> enc = preprocessing.OneHotEncoder()\n",
    ">>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
    ">>> enc.fit(X)\n",
    "OneHotEncoder()\n",
    ">>> enc.transform([['female', 'from US', 'uses Safari'],\n",
    "...                ['male', 'from Europe', 'uses Safari']]).toarray()\n",
    "array([[1., 0., 0., 1., 0., 1.],\n",
    "       [0., 1., 1., 0., 0., 1.]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Balancing\n",
    "- Weight balancing\n",
    "- Bagging\n",
    "- Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction\n",
    "Dimensionality reduction refers to techniques for reducing the number of input variables in training data. When dealing with high dimensional data, it is often useful to reduce the dimensionality by projecting the data to a lower dimensional subspace which captures the “essence” of the data.\n",
    "\n",
    "<p align=center>\n",
    "    <img src=\"./Assets/dimenReEx.png\" width=800>\n",
    "</p>\n",
    "\n",
    "\n",
    "In this example if the weight and height are dependent then we're basically asking the same question. What we need to do is make new varibles where we can make the covariance to be 0 then they will be independent statistically.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feautre extraction\n",
    "\n",
    "- PCA\n",
    "\n",
    "An important machine learning method for dimensionality reduction is called Principal Component Analysis.\n",
    "It is a method that uses simple matrix operations from linear algebra and statistics to calculate a projection of the original data into the same number or fewer dimensions.\n",
    "\n",
    "<p align=left>\n",
    "    <img src=\"./Assets/pcaaEx.png\" width=300>\n",
    "    <img src=\"./Assets/pcabEx.png\" width=400>\n",
    "</p>\n",
    "\n",
    "\n",
    "- LDA\n",
    "\n",
    "Linear Discriminant Analysis or LDA is a dimensionality reduction technique. It is used as a pre-processing step in Machine Learning and applications of pattern classification. The goal of LDA is to project the features in higher dimensional space onto a lower-dimensional space in order to avoid the curse of dimensionality and also reduce resources and dimensional costs.\n",
    "\n",
    "<p align=left>\n",
    "    <img src=\"./Assets/ldaEx.png\" width=300>\n",
    "</p>\n",
    "\n",
    "- ICA\n",
    "- Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve\n",
    "\n",
    "#### Learning Theory\n",
    " \n",
    " 1) Change model\n",
    " \n",
    " 2) Get more data\n",
    " \n",
    " 3) Not enough features\n",
    " \n",
    "- Overfit vs underfit\n",
    "\n",
    "<p align=center>\n",
    "    <img src=\"./Assets/overunderfitEx.png\" width=500>\n",
    "</p>\n",
    " \n",
    "#### Learning Curve\n",
    "A learning curve is a plot of model learning performance over experience or time.\n",
    "\n",
    "Learning curves are a widely used diagnostic tool in machine learning for algorithms that learn from a training dataset incrementally. The model can be evaluated on the training dataset and on a hold out validation dataset after each update during training and plots of the measured performance can created to show learning curves.\n",
    "\n",
    "Reviewing learning curves of models during training can be used to diagnose problems with learning, such as an underfit or overfit model, as well as whether the training and validation datasets are suitably representative.\n",
    "\n",
    "<p align=center>\n",
    "    <img src=\"./Assets/learningCurvesEx.png\" width=500>\n",
    "</p>\n",
    "\n",
    "<b>Bias</b> is the dotted line\n",
    "\n",
    "The <b>variance</b> ais how far they are from each other (the test and train).\n",
    "    \n",
    "<br/>\n",
    "\n",
    "In the picture the green model is high bias and low variance\n",
    "and the purple has low bias and high variance\n",
    "\n",
    "\n",
    "Lesson learned:\n",
    "Match the 'model complexity' to the data resources, not the target complexity.\n",
    "\n",
    "<p align=center>\n",
    "    <img src=\"./Assets/biasAndVarEx.png\" width=500>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "#### Regularization\n",
    "\n",
    "This is a form of regression, that constrains/ regularizes or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, so as to avoid the risk of overfitting. A simple relation for linear regression looks like this.\n",
    "\n",
    "<p align=center>\n",
    "    <img src=\"./Assets/regulatorEx.png\" width=500>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "What is Confusion Matrix and why you need it?\n",
    "Well, it is a performance measurement for machine learning classification problem where output can be two or more classes. It is a table with 4 different combinations of predicted and actual values.\n",
    "\n",
    "\n",
    "It is extremely useful for measuring Recall, Precision, Specificity, Accuracy, and most importantly AUC-ROC curves.\n",
    "\n",
    "<p align=center>\n",
    "    <img src=\"./Assets/confusionEx.png\" width=500>\n",
    "</p>\n",
    "\n",
    "#### ROC\n",
    "\n",
    "An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds\n",
    "\n",
    "<p align=center>\n",
    "    <img src=\"./Assets/roc_ex.png\" width=500>\n",
    "</p>\n",
    "\n",
    "#### Precision Recall\n",
    "\n",
    "Precision quantifies the number of positive class predictions that actually belong to the positive class. Recall quantifies the number of positive class predictions made out of all positive examples in the dataset. F-Measure provides a single score that balances both the concerns of precision and recall in one number.\n",
    "\n",
    "<p align=center>\n",
    "    <img src=\"./Assets/precisionEx.png\" width=500>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Selection\n",
    "When creating a machine learning model, you'll be presented with design choices as to how to define your model architecture. Often times, we don't immediately know what the optimal model architecture should be for a given model, and thus we'd like to be able to explore a range of possibilities. In true machine learning fashion, we'll ideally ask the machine to perform this exploration and select the optimal model architecture automatically. Parameters which define the model architecture are referred to as hyperparameters and thus this process of searching for the ideal model architecture is referred to as hyperparameter tuning.\n",
    "\n",
    "These hyperparameters might address model design questions such as:\n",
    "\n",
    "What degree of polynomial features should I use for my linear model?\n",
    "What should be the maximum depth allowed for my decision tree?\n",
    "What should be the minimum number of samples required at a leaf node in my decision tree?\n",
    "How many trees should I include in my random forest?\n",
    "How many neurons should I have in my neural network layer?\n",
    "How many layers should I have in my neural network?\n",
    "What should I set my learning rate to for gradient descent?\n",
    "\n",
    "<p align=center>\n",
    "    <img src=\"./Assets/gridVsRandomEx.png\" width=500>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Pitfalls\n",
    "### Overfitting\n",
    "Overfitting is when there are outliers and the model overfits to match the outliers, and usualy it peforms well on the test data but wont perform well on real data.\n",
    "<p align=center>\n",
    "    <img src=\"./Assets/overfittingEx.png\" width=400>\n",
    "</p>\n",
    "\n",
    "### Spilt Test & Train\n",
    "In many situations, simple train/test splits and cross-validation will yield misleading estimates.\n",
    "A naive train/test split implicitly assumes that our data consists of iid samples.\n",
    "If our data violates this iid assumption, then the test-set performance will mislead us—and usually cause us to overestimate our model’s real-world performance.\n",
    "This can lead to bad consequences: at the very least, it makes you (the data scientist) seem untrustworthy or incompetent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
